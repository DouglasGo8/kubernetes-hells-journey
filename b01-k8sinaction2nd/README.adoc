= Kubernetes In Action 2nd

== Links

- https://github.com/luksa/kubernetes-in-action-2nd-edition[Books GitRepo]
- https://camel.apache.org/components/2.x/kubernetes-component.html[Apache Camel Kubernetes API]
- https://github.com/Apress/cloud-native-integration-apache-camel[Cloud Native Integration with Apache Camel Git Source]
- https://stackoverflow.com/questions/62694361/how-to-reference-a-local-volume-in-kind-kubernetes-in-docker[KinD Volumes How-to]
- https://github.com/stacksimplify/aws-eks-kubernetes-masterclass[K8s Masterclass]
- https://github.com/stacksimplify/docker-fundamentals[Docker fundamentals]
- https://github.com/stacksimplify/kubernetes-fundamentals[K8s fundamentals]
- https://kubernetes.io/docs/reference/kubectl/cheatsheet/[kubeclt Cheat Sheet]

== Minikube Configuration

[source,bash]
----
minikube config set cpus 4
minikube config set memory 8192

minikube start
minikube start --driver=docker --alsologtostderr

minikube addons enable registry
minikube dashboard
minikube dashboard --url
# oc
kubectl create namespace k8s-inaction
kubectl config set-context --current --namespace=k8s-inaction
# get current ns
kubectl config view --minify -o jsonpath='{..namespace}'
kubectl get all,pvc,configmap,rolebindings,clusterrolebindings,secrets,sa,roles,clusterroles,crd -l 'app=my-label'
----

== Minishift OC env Configuration

* OC command in Minishift cluster env is the same kubectl command in K8's cluster

[source,bash]
----
minishift oc-env
eval $(minishift oc-env)
----

[source,bash]
----
brew upgrade minikube
----

=== Quarkus Template info

==== Call App services

[source,shellscript]
----
curl http://localhost:12080/api/greeting/meh.ho
----

==== Running the application in dev mode

You can run your application in dev mode that enables live coding using:

[source,shellscript]
----
./mvnw compile quarkus:dev
----

> **_NOTE:_**  Quarkus now ships with a Dev UI, which is available in dev mode only at http://localhost:8080/q/dev/.

==== Packaging and running the application

The application can be packaged using:

[source,shellscript]
----
./mvnw package
----

It produces the `quarkus-run.jar` file in the `target/quarkus-app/` directory.
Be aware that it’s not an _über-jar_ as the dependencies are copied into the `target/quarkus-app/lib/` directory.

The application is now runnable using `java -jar target/quarkus-app/quarkus-run.jar`.

If you want to build an _über-jar_, execute the following command:

[source,shellscript]
----
./mvnw package -Dquarkus.package.type=uber-jar
----

The application, packaged as an _über-jar_, is now runnable using `java -jar target/*-runner.jar`.

==== Creating a native executable

You can create a native executable using:

[source,shellscript]
----
./mvnw package -Pnative
----

Or, if you don't have GraalVM installed, you can run the native executable build in a container using:

[source,shellscript]
----
./mvnw package -Pnative -Dquarkus.native.container-build=true
----

You can then execute your native executable with: `./target/api-backend-v01-1.0.0-SNAPSHOT-runner`

If you want to learn more about building native executables, please consult https://quarkus.io/guides/maven-tooling[maven-tooling].

* Camel JSON Path https://camel.apache.org/camel-quarkus/latest/reference/extensions/jsonpath.html[guide]: Evaluate a JSONPath expression against a JSON message body
* Camel Core https://camel.apache.org/camel-quarkus/latest/reference/extensions/core.html[guide]: Camel core functionality and basic Camel languages: Constant, ExchangeProperty, Header, Ref, Simple and Tokenize
* Camel Direct https://camel.apache.org/camel-quarkus/latest/reference/extensions/direct.html[guide]: Call another endpoint from the same Camel Context synchronously
* Camel Jackson https://camel.apache.org/camel-quarkus/latest/reference/extensions/jackson.html[guide]: Marshal POJOs to JSON and back using Jackson
* Camel Bean https://camel.apache.org/camel-quarkus/latest/reference/extensions/bean.html[guide]: Invoke methods of Java beans
* Camel Rest https://camel.apache.org/camel-quarkus/latest/reference/extensions/rest.html[guide]: Expose REST services and their OpenAPI Specification or call external REST services

== Kubernetes in Action 2nd

=== Introduction Kubernetes API

.A K8s cluster objects
image::architecture/thumbs/api_k8s_interaction.jpg[]

* They include the applications running in the cluster, their configuration, the load balancers through which they are exposed within the cluster or externally, the underlying servers and the storage used by these applications, the security privileges of users and applications, and many other details of the infrastructure.
* The collection of all deployments in the cluster is a REST resource exposed at ##/api/v1/deployments##
* The Kubernetes Control Plane runs several components called controllers that manage the objects you create.
Each controller is usually only responsible for one object type.
For example, the Deployment controller manages Deployment objects.

[source,bash]
----
kubectl get notes
kubectl get node localhost -o yaml
----

* To use API we need expose the api

[source,bash]
----
kubectl proxy
----

=== Running workloads in Pods

.Three basic object types comprising a deployed application
image::architecture/thumbs/comprising_deploy_k8s_pod.jpg[]

. World -> Service -> Pods <- -> Deployment

. Pod object, represents the ##central and most important concept in K8s## - a running instance of your application, in the K8s we don't deploy containers but groups of them managed as a single unit, ##When a Pod has multiple containers, all of them run on the same worker node##, a single pod instance never spans multiple nodes,
. Containers are _designed_ to run only a single process, to take full advantage of the features provided by the container runtime, you should consider running only one process in each container.
. With a pod, you can run closely related processes together, giving them (almost) the same environment as if they were all running in a single container, all containers in a pod share the same Network namespace and thus the network interfaces, IP address(es) and port space that belong to it.
Because of the shared port space, processes running in containers of the same pod can’t be bound to the same port numbers, All the containers in a pod also see the same system hostname.
. Each pod can be considered as separated vCPu/Cpu, avoid stuffing all your applications into a single pod, you should divide them so that each pod runs only closely related application processes.
. ##Kubernetes does not replicate containers within a pod.
It replicates the entire pod.##
. You never need to combine multiple applications in a single pod

.Containers in a pod sharing the same Network namespace
image::architecture/thumbs/pod_share_same_network_interface.jpg[]

==== Sidecar Containers

. Several containers in a single pod is only appropriate if the application consists of a primary process and one or more processes that complement the operation of the primary process.

.Sidecar Container in same Pod
image::architecture/thumbs/sidecar.png[]

==== How to decide whether to split containers into multiple pods

When deciding whether to use the sidecar pattern and place containers in a single pod, or to place them in separate pods, ask yourself the following questions:

* Do these containers have to run on the same host?
* Do I want to manage them as a single unit?
* Do they form a unified whole instead of being independent components?
* Do they have to be scaled together?
* Can a single node meet their combined resource needs?

.Create Manifest suing kubectl
[source,shellscript]
----
kubectl explain pods
kubectl run mypod --image=tag/image:1.0 -dry-run=client -o yaml > mypod.yaml
----

.Sample Pod Running
image::architecture/thumbs/pod-running.png[]

==== Create/Handle Pod Object

[source,bash]
----
oc apply -f pod-kiada.yaml
oc get pod kiada
oc describe pod kiada
oc get events
oc get pod kiada -o wide
oc run --image=curlimages/curl -it --restart=Never --rm client-pod curl {{IP_POD}}:8080
oc port-forward kiada 8080
oc logs kiada
oc logs kiada -f
oc logs kiada --timestamps=true
oc logs kiada --since=2m
oc logs kiada --since-time=2020-02-01T09:50:00Z
oc exec kiada -- ps aux
oc exec kiada -- curl -s localhost:8080
oc exec kiada curl -s localhost:8080
oc exec -it kiada -- bash
oc attach kiada
oc attach -i kiada-stdin # version two using stdin image

oc port-forward kiada-ssl 8080 8443 990
oc logs kiada-ssl -c kiada
oc logs kiada-ssl --all-containers
oc get pods -w # watch status changing

oc delete po kiada
oc delete po --all
oc delete -f pod.kiada.yaml,pod.kiada-ssl.yaml
----

=== Copying files to and from containers

[source,bash]
----
oc cp kiada:html/index.html /tmp/index.html
oc cp /tmp/index.html kiada:html/
----

== Sidecar pattern

.Sidecar pattern One Pod Two Containers
image::architecture/thumbs/sidecarpattern.png[]

[source,bash]
----
curl https://example.com:8443 --resolve example.com:8443:127.0.0.1 --cacert kiada-ssl-proxy-0.1/example-com.crt
----

== Pod Lifecycles

[source,bash]
----
oc get po kiada -o yaml | grep phase
oc get pods -n myproject
oc describe po kiada
oc get po kiada -o json | jq .status.conditions
[{
    "lastProbeTime": null,
    "lastTransitionTime": "2020-02-02T11:42:59Z",
    "status": "True",
    "type": "Initialized"
  }]
oc get po kiada -o json | jq .status
oc get pods -w
oc get events -w
oc logs kiada-liveness -c kiada -f
oc exec kiada-liveness -c envoy -- tail -f /tmp/envoy.admin.log
curl -X POST localhost:9901/healthcheck/fail
kubectl get po kiada-ssl -o json | jq .status.containerStatuses
----

* restartPolicy - default #_Always_# | #_OnFailure_# | #_Never_#

In a long startup app scenario, you can increase the initialDelaySeconds, periodSeconds or failureThreshold

[source,yaml]
----
  containers:
  - name: kiada
    image: luksa/kiada:0.1
    ports:
    - name: http
      containerPort: 8080
    startupProbe:
      httpGet:
        path: /
        port: http
      periodSeconds: 10
      failureThreshold:  12
    livenessProbe:
      httpGet:
        path: /
        port: http
      periodSeconds: 5
      failureThreshold: 2
----

. Lifecycle hooks, pre-start and pre-stop

== Attaching storage Volumes to Pods

. We've 3 possibilities of volume creation,
.. Container's volume, isolated filesystem
.. Pod's volume, can be shared with specific permissions
.. External's volume, cross Pod lifecycles

. When you add a volume to a pod, you must specify the volume type, they are:

.. *empty_dir* The simplest volume type, is a directory that allows the pod to store data for the duration of its life cycle.
.. *hostPath* Used for mounting files from the worker node’s filesystem into the pod
.. *nfs* An NFS share mounted into the pod
.. *gcePersistentDisk, awsElasticBlockStore, azureFile, azureDisk*
.. *configMap, secret, downwardAPI**
.. *persistentVolumeClaim* A portable way to integrate external storage into pods.
Instead of pointing directly to an external storage volume


.Mounting a filesystem into the file tree
image::architecture/thumbs/attaching_dir_structure.png[]

.A volume mounted into more than one container
image::architecture/thumbs/attached_volume_shared.png[]

[source,bash]
----
kubectl exec -it quiz -c mongo -- mongo
----

== Persistent Volumes and Claims

. To make pod manifests portable across different clusters envs, we need of an abstract way to claims storage definitions, a _PersistentVolumeClaim_ object connects the pod to this PersistentVolume object

.Persistent Volume Claim
image::architecture/thumbs/pvc.png[]

.Reading a crt file in a secret volume
[source,bash]
----
kubectl exec pod-name -c container-name -- cat /etc/certs/example-com.crt
----

== Exposing Pods with Services

.Pods communication
image::architecture/thumbs/pods_communications.png[]

When a pod sends a network packet to another pod, neither SNAT (Source NAT) nor DNAT (Destination NAT) is performed on the packet.
This means that the source IP and port, and the destination IP and port, of packets exchanged directly between pods are never changed.
If the sending pod knows the IP address of the receiving pod, it can send packets to it.
The receiving pod can see the sender’s IP as the source IP address of the packet.

Although there are many Kubernetes network plugins, they must all behave as described above.
Therefore, the communication between two pods is always the same, regardless of whether the pods are running on the same node or on nodes located in different geographic regions.
The containers in the pods can communicate with each other over the flat NAT-less network, like computers on a local area network (LAN) connected to a single network switch.
From the perspective of the applications, the actual network topology between the nodes isn’t important

.Service Object over Pods
image::architecture/thumbs/service_load_balance_over_pods.png[]

.Flow Service Pods
image::architecture/thumbs/flow-service-pods.png[]

[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: quote
spec:
  type: ClusterIP # Only Cluster Communication
  selector:
    app: quote
  ports:
    - port: 80
      targetPort: 80
      protocol: TCP
----

[source,bash]
----
kubectl get svc -o wide
kubectl set selector service quiz app=quiz
# expose ClusterIP Pod/Service
kubectl exec -it {{pod_name}} -c {{container_name}} -- sh
# expose env vars
kubectl exec -it {{pod_name}} -c {{container_name}} -- env | sort

----

A service is resolvable under the following DNS names:

* <service-name>, if the service is in the same namespace as the pod performing the DNS lookup,
* <service-name>.<service-namespace> from any namespace, but also under
* <service-name>.<service-namespace>.svc, and
* <service-name>.<service-namespace>.svc.cluster.local
